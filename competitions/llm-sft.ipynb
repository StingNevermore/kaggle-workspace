{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-23T03:32:39.551442Z",
     "start_time": "2024-10-23T03:32:21.557076Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llm-sft\"\n",
    "\n",
    "model_identifier = \"Qwen2.5-7B-Instruct\"\n",
    "model_name = f\"Qwen/{model_identifier}\"\n",
    "data_dir = \"data/llm-sft\"\n",
    "models_dir = os.path.join(data_dir, \"models\", model_identifier)\n",
    "!modelscope download --model $model_name --local_dir $models_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7824f5c2bc3a4a8bbe1c4d82673495b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    models_dir, torch_dtype=torch.bfloat16, device_map=\"cuda\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat <<'EOT' > ds_config_zero3.json\n",
    "{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupCosineLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_num_steps\": \"auto\",\n",
    "            \"total_num_steps\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 1,\n",
    "        \"overlap_comm\": true,\n",
    "        \"contiguous_gradients\": true,\n",
    "        \"sub_group_size\": 1e9,\n",
    "        \"reduce_bucket_size\": \"auto\"\n",
    "    },\n",
    "\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\n",
    "EOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import concatenate_datasets, load_dataset\n",
    "\n",
    "DEFAULT_SYSTEM_PROMPT = (\n",
    "    \"你是一个机器人助手，你需要尽可能准确、简单的回复用户的所有问题。\"\n",
    ")\n",
    "\n",
    "batch_size = 1\n",
    "max_length = 512\n",
    "fin_dataset = load_dataset(\"Maciel/FinCUGE-Instruction\")\n",
    "# medi_dataset = load_dataset(\"SylvanL/Traditional-Chinese-Medicine-Dataset-SFT\")\n",
    "\n",
    "SOURCE_TEMPLATE = \"<|im_start|>system\\n{system}<|im_end|>\\n<|im_start|>user\\n{user}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "TARGET_TEMPLATE = \"{assistant}<|endoftext|>\"\n",
    "\n",
    "\n",
    "def infer_seqlen(source_len: int, target_len: int, cutoff_len: int):\n",
    "    r\"\"\"\n",
    "    Computes the real sequence length after truncation by the cutoff_len.\n",
    "    \"\"\"\n",
    "    if target_len * 2 < cutoff_len:  # truncate source\n",
    "        max_target_len = cutoff_len\n",
    "    elif source_len * 2 < cutoff_len:  # truncate target\n",
    "        max_target_len = cutoff_len - source_len\n",
    "    else:  # truncate both\n",
    "        max_target_len = int(cutoff_len * (target_len / (source_len + target_len)))\n",
    "\n",
    "    new_target_len = min(max_target_len, target_len)\n",
    "    max_source_len = max(cutoff_len - new_target_len, 0)\n",
    "    new_source_len = min(max_source_len, source_len)\n",
    "    return new_source_len, new_target_len\n",
    "\n",
    "\n",
    "def preprocess_dataset(dataset, all_columns, instruction_col, input_col, output_col):\n",
    "\n",
    "    def preprocess(examples):\n",
    "        sources = []\n",
    "        targets = []\n",
    "\n",
    "        if instruction_col is not None:\n",
    "            iter_t = (\n",
    "                examples[instruction_col],\n",
    "                examples[input_col],\n",
    "                examples[output_col],\n",
    "            )\n",
    "        else:\n",
    "            iter_t = examples[input_col], examples[output_col]\n",
    "\n",
    "        for text in zip(*iter_t):\n",
    "            if instruction_col is not None:\n",
    "                instruction, input, output = text\n",
    "                input = instruction + input\n",
    "            else:\n",
    "                input, output = text\n",
    "            source_message = SOURCE_TEMPLATE.format(\n",
    "                system=DEFAULT_SYSTEM_PROMPT, user=input\n",
    "            )\n",
    "            target_message = TARGET_TEMPLATE.format(assistant=output)\n",
    "            source = tokenizer(\n",
    "                source_message,\n",
    "                return_attention_mask=False,\n",
    "                add_special_tokens=False,\n",
    "            )[\"input_ids\"]\n",
    "            target = tokenizer(\n",
    "                target_message,\n",
    "                add_special_tokens=False,\n",
    "                return_attention_mask=False,\n",
    "            )[\"input_ids\"]\n",
    "            source_len, target_len = infer_seqlen(len(source), len(target), max_length)\n",
    "            source, target = source[:source_len], target[:target_len]\n",
    "            sources.append(source + target)\n",
    "            targets.append([-100] * source_len + target)\n",
    "        return {\"input_ids\": sources, \"labels\": targets}\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        num_proc=16,\n",
    "        remove_columns=all_columns,\n",
    "        keep_in_memory=False,\n",
    "    )\n",
    "\n",
    "\n",
    "fin_dataset = preprocess_dataset(\n",
    "    fin_dataset, fin_dataset[\"train\"].column_names, \"instruction\", \"input\", \"output\"\n",
    ")\n",
    "# medi_dataset = preprocess_dataset(\n",
    "#     medi_dataset, medi_dataset[\"train\"].column_names, \"instruction\", \"input\", \"output\"\n",
    "# )\n",
    "# medi_dataset = medi_dataset.shuffle(seed=42)\n",
    "# medi_dataset = medi_dataset[\"train\"].shard(num_shards=100, index=0)\n",
    "# dataset = concatenate_datasets([fin_dataset[\"train\"], medi_dataset]).shuffle(seed=42)\n",
    "fin_dataset = fin_dataset[\"train\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 40,370,176 || all params: 7,655,986,688 || trainable%: 0.5273\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-22 13:43:14,077] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-22 13:43:15,003] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-10-22 13:43:15,004] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/nevermore/.cache/torch_extensions/py310_cu124 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/nevermore/.cache/torch_extensions/py310_cu124/fused_adam/build.ninja...\n",
      "/home/nevermore/ml-workspace/kaggle-workspace/.venv/lib/python3.10/site-packages/torch/utils/cpp_extension.py:1965: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module fused_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load fused_adam op: 0.055814504623413086 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msting_nevermore\u001b[0m (\u001b[33msting_nevermore-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nevermore/ml-workspace/kaggle-workspace/wandb/run-20241022_134320-vls5pt01</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sting_nevermore-personal/llm-sft/runs/vls5pt01' target=\"_blank\">Qwen/Qwen2.5-7B-Instruct-fin</a></strong> to <a href='https://wandb.ai/sting_nevermore-personal/llm-sft' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sting_nevermore-personal/llm-sft' target=\"_blank\">https://wandb.ai/sting_nevermore-personal/llm-sft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sting_nevermore-personal/llm-sft/runs/vls5pt01' target=\"_blank\">https://wandb.ai/sting_nevermore-personal/llm-sft/runs/vls5pt01</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/nevermore/ml-workspace/kaggle-workspace/.venv/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15392' max='15392' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15392/15392 5:53:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>1.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>462</td>\n",
       "      <td>0.686100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>616</td>\n",
       "      <td>0.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.585300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>924</td>\n",
       "      <td>0.612600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1078</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1232</td>\n",
       "      <td>0.603100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1386</td>\n",
       "      <td>0.559100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>0.541100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1694</td>\n",
       "      <td>0.555000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1848</td>\n",
       "      <td>0.521900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2002</td>\n",
       "      <td>0.582800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2156</td>\n",
       "      <td>0.561300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>0.553500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2464</td>\n",
       "      <td>0.546800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2618</td>\n",
       "      <td>0.525100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2772</td>\n",
       "      <td>0.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2926</td>\n",
       "      <td>0.512800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>0.574900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3234</td>\n",
       "      <td>0.478500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3388</td>\n",
       "      <td>0.548200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3542</td>\n",
       "      <td>0.486800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3696</td>\n",
       "      <td>0.532800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>0.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4004</td>\n",
       "      <td>0.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4158</td>\n",
       "      <td>0.492800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4312</td>\n",
       "      <td>0.516200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4466</td>\n",
       "      <td>0.481800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>0.538500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4774</td>\n",
       "      <td>0.490800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4928</td>\n",
       "      <td>0.490300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5082</td>\n",
       "      <td>0.521800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5236</td>\n",
       "      <td>0.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5390</td>\n",
       "      <td>0.502900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5544</td>\n",
       "      <td>0.495600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5698</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5852</td>\n",
       "      <td>0.507700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6006</td>\n",
       "      <td>0.473900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>0.495200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6314</td>\n",
       "      <td>0.476800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6468</td>\n",
       "      <td>0.502000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6622</td>\n",
       "      <td>0.498800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6776</td>\n",
       "      <td>0.436500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6930</td>\n",
       "      <td>0.485200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7084</td>\n",
       "      <td>0.506900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7238</td>\n",
       "      <td>0.495900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7392</td>\n",
       "      <td>0.475900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7546</td>\n",
       "      <td>0.503500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.510600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7854</td>\n",
       "      <td>0.478000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8008</td>\n",
       "      <td>0.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8162</td>\n",
       "      <td>0.526600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8316</td>\n",
       "      <td>0.505500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8470</td>\n",
       "      <td>0.438000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8624</td>\n",
       "      <td>0.516700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8778</td>\n",
       "      <td>0.482900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8932</td>\n",
       "      <td>0.453200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9086</td>\n",
       "      <td>0.469900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>0.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9394</td>\n",
       "      <td>0.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9548</td>\n",
       "      <td>0.460000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9702</td>\n",
       "      <td>0.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9856</td>\n",
       "      <td>0.472400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10010</td>\n",
       "      <td>0.443600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10164</td>\n",
       "      <td>0.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10318</td>\n",
       "      <td>0.460800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10472</td>\n",
       "      <td>0.467000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10626</td>\n",
       "      <td>0.474300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10780</td>\n",
       "      <td>0.493100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10934</td>\n",
       "      <td>0.494100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11088</td>\n",
       "      <td>0.451400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11242</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11396</td>\n",
       "      <td>0.479100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11550</td>\n",
       "      <td>0.446700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11704</td>\n",
       "      <td>0.403600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11858</td>\n",
       "      <td>0.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12012</td>\n",
       "      <td>0.488900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12166</td>\n",
       "      <td>0.425600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12320</td>\n",
       "      <td>0.482800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12474</td>\n",
       "      <td>0.478600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12628</td>\n",
       "      <td>0.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12782</td>\n",
       "      <td>0.462800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12936</td>\n",
       "      <td>0.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13090</td>\n",
       "      <td>0.439100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13244</td>\n",
       "      <td>0.480900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13398</td>\n",
       "      <td>0.453000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13552</td>\n",
       "      <td>0.483800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13706</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13860</td>\n",
       "      <td>0.487300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14014</td>\n",
       "      <td>0.448600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14168</td>\n",
       "      <td>0.459400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14322</td>\n",
       "      <td>0.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14476</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14630</td>\n",
       "      <td>0.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14784</td>\n",
       "      <td>0.457000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14938</td>\n",
       "      <td>0.490200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15092</td>\n",
       "      <td>0.462200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15246</td>\n",
       "      <td>0.444900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15392, training_loss=0.5109334103530757, metrics={'train_runtime': 21238.0598, 'train_samples_per_second': 5.798, 'train_steps_per_second': 0.725, 'total_flos': 7.12519465014657e+17, 'train_loss': 0.5109334103530757, 'epoch': 0.9999918789640807})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "train_dataset = fin_dataset\n",
    "\n",
    "args = TrainingArguments(\n",
    "    deepspeed=\"ds_config_zero3.json\",\n",
    "    output_dir=os.path.join(data_dir, \"outputs\", model_name),\n",
    "    logging_dir=os.path.join(data_dir, \"logs\", model_name),\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    save_only_model=True,\n",
    "    save_steps=0.2,\n",
    "    logging_steps=0.01,\n",
    "    report_to=[\"wandb\"],\n",
    "    run_name=f\"{model_name}-fin\",\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    max_grad_norm=1,\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_num_workers=16,\n",
    "    dataloader_pin_memory=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model.save_pretrained(os.path.join(data_dir, \"models\", f\"{model_identifier}-lora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# from peft import PeftModel\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     model_id=\"/home/nevermore/model_deploy/Qwen-7B/lora\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "你是一个机器人助手，你需要尽可能准确、简单的回复用户的所有问题。\n",
      "user\n",
      "请评估以下内容的整体情感倾向，包含积极、消极和中性。他妈的出不完的货\n",
      "assistant\n",
      "所属情感是消极。\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": DEFAULT_SYSTEM_PROMPT,\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"请评估以下内容的整体情感倾向，包含积极、消极和中性。他妈的出不完的货\"\"\",\n",
    "    },\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ").to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=1024,\n",
    "        top_p=0.8,\n",
    "        top_k=20,\n",
    "        temperature=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_module = __import__(\"scripts.long_seq_classifier\", fromlist=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "950ee8d7ad4d47bca9134f345d6dc137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LongSeqClassifier(\n",
       "  (base_model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 3072)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "          (down_proj): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((3072,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (word_lstm): LSTM(3072, 256, batch_first=True, bidirectional=True)\n",
       "  (sentence_lstm): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (classifier): Linear(in_features=512, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getattr(classifier_module, \"get_model\", None)(\"meta-llama/llama-3.2-3B-Instruct\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
