{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = \"data/readability\"\n",
    "model_out_dir = os.path.join(data_dir, \"model_out\")\n",
    "train_csv = pd.read_csv(os.path.join(data_dir, \"train.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn import model_selection\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "class ClassificationDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.tokens = tokenizer(\n",
    "            data[\"excerpt\"].tolist(),\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if \"target\" in data.columns:\n",
    "            self.lables = data[\"target\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens[\"input_ids\"])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return_dict = {\n",
    "            \"input_ids\": self.tokens[\"input_ids\"][index],\n",
    "            \"attention_mask\": self.tokens[\"attention_mask\"][index],\n",
    "        }\n",
    "        if \"token_type_ids\" in self.tokens:\n",
    "            return_dict.update({\"token_type_ids\": self.tokens[\"token_type_ids\"][index]})\n",
    "        if hasattr(self, \"lables\"):\n",
    "            return_dict.update({\"labels\": torch.tensor(self.lables[index])})\n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class BertBasedClassifier(nn.Module):\n",
    "    def __init__(self, bert, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        bert_output_size = bert.config.hidden_size\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(bert_output_size, 512),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "        self.regressor = nn.Linear(bert_output_size, 1)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, token_type_ids=None, attention_mask=None, labels=None\n",
    "    ):\n",
    "        if token_type_ids is None:\n",
    "            bert_output = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "        elif attention_mask is None:\n",
    "            bert_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        else:\n",
    "            bert_output = self.bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids,\n",
    "            )\n",
    "        last_layer_hidden_states = bert_output.last_hidden_state\n",
    "        attention = self.attention(last_layer_hidden_states)\n",
    "        output = torch.sum(attention * last_layer_hidden_states, dim=1)\n",
    "        logits = self.regressor(self.dropout(output))\n",
    "        if labels is None:\n",
    "            return logits\n",
    "        loss_fn = torch.nn.MSELoss()\n",
    "        loss = loss_fn(logits.reshape(labels.shape), labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, Trainer, TrainerCallback, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CollateMetricsCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rmse = 0.0\n",
    "\n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        self.rmse += state.best_metric\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    model_name,\n",
    "    train_data,\n",
    "    eval_data=None,\n",
    "    output_dir=None,\n",
    "    model_save_dir=None,\n",
    "    logging_dir=None,\n",
    "    batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    learning_rate=5e-6,\n",
    "    trainer_callback=CollateMetricsCallback(),\n",
    "):\n",
    "    torch.cuda.empty_cache()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    output_dir = os.path.join(\n",
    "        model_out_dir, f\"{model_name}-output\" if output_dir is None else output_dir\n",
    "    )\n",
    "    model_save_dir = os.path.join(\n",
    "        model_out_dir,\n",
    "        f\"{model_name}-model\" if model_save_dir is None else model_save_dir,\n",
    "    )\n",
    "    logging_dir = os.path.join(\n",
    "        model_out_dir, f\"{model_name}-runs\" if logging_dir is None else logging_dir\n",
    "    )\n",
    "    print(\n",
    "        f\"output_dir: {output_dir}, model_save_dir: {model_save_dir}, logging_dir: {logging_dir}\"\n",
    "    )\n",
    "    train_dataset = ClassificationDataset(train_data, tokenizer)\n",
    "    if eval_data is not None:\n",
    "        eval_dataset = ClassificationDataset(eval_data, tokenizer)\n",
    "        args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            logging_dir=logging_dir,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            eval_strategy=\"steps\",\n",
    "            logging_steps=0.01,\n",
    "            save_steps=0.2,\n",
    "            eval_steps=0.2,\n",
    "            # torch_empty_cache_steps=1,\n",
    "            weight_decay=0.01,\n",
    "            load_best_model_at_end=True,\n",
    "            greater_is_better=False,\n",
    "            metric_for_best_model=\"eval_rmse\",\n",
    "            save_only_model=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            compute_metrics=lambda x: {\n",
    "                \"rmse\": np.sqrt(\n",
    "                    np.mean(\n",
    "                        (x.predictions.reshape(x.label_ids.shape) - x.label_ids) ** 2\n",
    "                    )\n",
    "                )\n",
    "            },\n",
    "            callbacks=[trainer_callback],\n",
    "        )\n",
    "    else:\n",
    "        args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            logging_dir=logging_dir,\n",
    "            report_to=[\"tensorboard\"],\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=0.01,\n",
    "            eval_steps=0.2,\n",
    "            save_steps=0.2,\n",
    "            # torch_empty_cache_steps=1,\n",
    "            weight_decay=0.01,\n",
    "            save_only_model=True,\n",
    "            lr_scheduler_type=\"cosine\",\n",
    "            warmup_ratio=0.1,\n",
    "        )\n",
    "        trainer = Trainer(\n",
    "            args=args,\n",
    "            model=model,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_save_dir)\n",
    "    return trainer_callback.rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bert-large-cased....\n",
      "output_dir: data/readability/model_out/bert-large-cased-output, model_save_dir: data/readability/model_out/bert-large-cased-model, logging_dir: data/readability/model_out/bert-large-cased-runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='852' max='852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [852/852 02:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.439300</td>\n",
       "      <td>0.393092</td>\n",
       "      <td>0.626970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.221900</td>\n",
       "      <td>0.315312</td>\n",
       "      <td>0.561527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>513</td>\n",
       "      <td>0.188500</td>\n",
       "      <td>0.300294</td>\n",
       "      <td>0.547991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>684</td>\n",
       "      <td>0.190100</td>\n",
       "      <td>0.276998</td>\n",
       "      <td>0.526306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training albert-xxlarge-v2....\n",
      "output_dir: data/readability/model_out/albert-xxlarge-v2-output, model_save_dir: data/readability/model_out/albert-xxlarge-v2-model, logging_dir: data/readability/model_out/albert-xxlarge-v2-runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/1420 15:58, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.389800</td>\n",
       "      <td>0.337839</td>\n",
       "      <td>0.581239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.292100</td>\n",
       "      <td>0.283436</td>\n",
       "      <td>0.532387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>0.203700</td>\n",
       "      <td>0.267793</td>\n",
       "      <td>0.517488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>0.260339</td>\n",
       "      <td>0.510235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.092900</td>\n",
       "      <td>0.259897</td>\n",
       "      <td>0.509801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training roberta-large....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: data/readability/model_out/roberta-large-output, model_save_dir: data/readability/model_out/roberta-large-model, logging_dir: data/readability/model_out/roberta-large-runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/1420 03:18, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.367900</td>\n",
       "      <td>0.308041</td>\n",
       "      <td>0.555014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.274900</td>\n",
       "      <td>0.240110</td>\n",
       "      <td>0.490010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>0.125700</td>\n",
       "      <td>0.242153</td>\n",
       "      <td>0.492090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>0.246342</td>\n",
       "      <td>0.496328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.057900</td>\n",
       "      <td>0.244904</td>\n",
       "      <td>0.494877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training deberta-v2-xlarge....\n",
      "output_dir: data/readability/model_out/deberta-v2-xlarge-output, model_save_dir: data/readability/model_out/deberta-v2-xlarge-model, logging_dir: data/readability/model_out/deberta-v2-xlarge-runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5670' max='5670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5670/5670 21:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1134</td>\n",
       "      <td>0.336100</td>\n",
       "      <td>0.442573</td>\n",
       "      <td>0.665261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2268</td>\n",
       "      <td>0.260100</td>\n",
       "      <td>0.370421</td>\n",
       "      <td>0.608622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3402</td>\n",
       "      <td>0.200200</td>\n",
       "      <td>0.246320</td>\n",
       "      <td>0.496307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4536</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.225542</td>\n",
       "      <td>0.474913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5670</td>\n",
       "      <td>0.083000</td>\n",
       "      <td>0.221810</td>\n",
       "      <td>0.470967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training muppet-roberta-large....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at facebook/muppet-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_dir: data/readability/model_out/muppet-roberta-large-output, model_save_dir: data/readability/model_out/muppet-roberta-large-model, logging_dir: data/readability/model_out/muppet-roberta-large-runs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1420' max='1420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1420/1420 03:15, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.427800</td>\n",
       "      <td>0.311388</td>\n",
       "      <td>0.558021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>568</td>\n",
       "      <td>0.278200</td>\n",
       "      <td>0.280583</td>\n",
       "      <td>0.529701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>852</td>\n",
       "      <td>0.156400</td>\n",
       "      <td>0.270674</td>\n",
       "      <td>0.520263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1136</td>\n",
       "      <td>0.073100</td>\n",
       "      <td>0.256173</td>\n",
       "      <td>0.506135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>0.052300</td>\n",
       "      <td>0.254979</td>\n",
       "      <td>0.504954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wrapped_bert(bert_name, hidden_dropout_prob=None):\n",
    "    def init_model():\n",
    "        if hidden_dropout_prob is not None:\n",
    "            bert = AutoModel.from_pretrained(\n",
    "                bert_name,\n",
    "                hidden_dropout_prob=hidden_dropout_prob,\n",
    "            )\n",
    "        else:\n",
    "            bert = AutoModel.from_pretrained(bert_name)\n",
    "        return BertBasedClassifier(bert)\n",
    "\n",
    "    return init_model\n",
    "\n",
    "\n",
    "train_data, eval_data = model_selection.train_test_split(\n",
    "    train_csv, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "hyperparameters = {\n",
    "    \"bert-large-cased\": {\n",
    "        \"model_init\": wrapped_bert(\"bert-large-cased\", hidden_dropout_prob=0.0),\n",
    "        \"train_params\": {\n",
    "            \"batch_size\": 8,\n",
    "            \"num_train_epochs\": 3,\n",
    "        },\n",
    "    },\n",
    "    \"albert-xxlarge-v2\": {\n",
    "        \"model_init\": wrapped_bert(\"albert-xxlarge-v2\", hidden_dropout_prob=0.0),\n",
    "    },\n",
    "    \"roberta-large\": {\n",
    "        \"model_init\": wrapped_bert(\"roberta-large\", hidden_dropout_prob=0.0),\n",
    "    },\n",
    "    \"deberta-v2-xlarge\": {\n",
    "        \"model_name\": \"microsoft/deberta-v2-xlarge\",\n",
    "        \"model_init\": wrapped_bert(\n",
    "            \"microsoft/deberta-v2-xlarge\", hidden_dropout_prob=0.0\n",
    "        ),\n",
    "        \"train_params\": {\n",
    "            \"batch_size\": 2,\n",
    "            \"num_train_epochs\": 5,\n",
    "            \"output_dir\": \"deberta-v2-xlarge-output\",\n",
    "            \"model_save_dir\": \"deberta-v2-xlarge-model\",\n",
    "            \"logging_dir\": \"deberta-v2-xlarge-runs\",\n",
    "        },\n",
    "    },\n",
    "    \"muppet-roberta-large\": {\n",
    "        \"model_name\": \"facebook/muppet-roberta-large\",\n",
    "        \"model_init\": wrapped_bert(\n",
    "            \"facebook/muppet-roberta-large\", hidden_dropout_prob=0.0\n",
    "        ),\n",
    "        \"train_params\": {\n",
    "            \"output_dir\": \"muppet-roberta-large-output\",\n",
    "            \"model_save_dir\": \"muppet-roberta-large-model\",\n",
    "            \"logging_dir\": \"muppet-roberta-large-runs\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def model_select(hyperparameters):\n",
    "    metrics = {}\n",
    "    for key, value in hyperparameters.items():\n",
    "        print(f\"Training {key}....\")\n",
    "        model = value[\"model_init\"]()\n",
    "        model_name = value.get(\"model_name\", key)\n",
    "        train_params = value.get(\"train_params\", {})\n",
    "        train(\n",
    "            model,\n",
    "            model_name,\n",
    "            train_data,\n",
    "            eval_data,\n",
    "            **train_params,\n",
    "        )\n",
    "    return pd.DataFrame(metrics.items(), columns=[\"model\", \"best_rmse\"])\n",
    "\n",
    "\n",
    "select_results = model_select(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at facebook/muppet-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from safetensors.torch import load_model\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "labels = eval_data[\"target\"].values\n",
    "for key, value in hyperparameters.items():\n",
    "    model = value[\"model_init\"]()\n",
    "    model_name = value.get(\"model_name\", key)\n",
    "    model_save_dir = os.path.join(model_out_dir, f\"{key}-model\")\n",
    "    load_model(model, os.path.join(model_save_dir, \"model.safetensors\"))\n",
    "    model.eval()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    eval_dataloader = DataLoader(\n",
    "        ClassificationDataset(eval_data, tokenizer), batch_size=2, shuffle=False\n",
    "    )\n",
    "    model.to(device)\n",
    "    all_logits = []\n",
    "    for inputs in eval_dataloader:\n",
    "        inputs.pop(\"labels\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs)\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "    all_logits = np.concatenate(all_logits).flatten()\n",
    "    pd.DataFrame({\"labels\": labels, \"target\": all_logits}).to_csv(\n",
    "        os.path.join(model_out_dir, f\"{key}-eval.csv\"), index=False\n",
    "    )\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       message: Optimization terminated successfully.\n",
       "       success: True\n",
       "        status: 0\n",
       "           fun: 0.461673542701431\n",
       "             x: [ 2.076e-01  2.064e-01  2.027e-01  1.911e-01  1.932e-01]\n",
       "           nit: 49\n",
       "          nfev: 98\n",
       " final_simplex: (array([[ 2.076e-01,  2.064e-01, ...,  1.911e-01,\n",
       "                         1.932e-01],\n",
       "                       [ 2.076e-01,  2.064e-01, ...,  1.912e-01,\n",
       "                         1.932e-01],\n",
       "                       ...,\n",
       "                       [ 2.076e-01,  2.064e-01, ...,  1.912e-01,\n",
       "                         1.932e-01],\n",
       "                       [ 2.077e-01,  2.064e-01, ...,  1.912e-01,\n",
       "                         1.932e-01]]), array([ 4.617e-01,  4.617e-01,  4.617e-01,  4.617e-01,\n",
       "                        4.617e-01,  4.617e-01]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "\n",
    "all_logits = []\n",
    "for key, value in hyperparameters.items():\n",
    "    eval_csv = pd.read_csv(os.path.join(model_out_dir, f\"{key}-eval.csv\"))\n",
    "    eval_logits = eval_csv[\"target\"].values\n",
    "    all_logits.append(eval_logits)\n",
    "\n",
    "\n",
    "def ensemble(weights, *logits):\n",
    "    return np.sum(\n",
    "        np.concatenate([w * np.array(l) for w, l in zip(weights, logits)], axis=1),\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "def weight_sum_loss(weights):\n",
    "    return root_mean_squared_error(labels, ensemble(weights, all_logits))\n",
    "\n",
    "\n",
    "init_guess = [1.0 / len(all_logits)] * len(all_logits)\n",
    "minimize(weight_sum_loss, init_guess, method=\"Nelder-Mead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at facebook/muppet-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "for key, value in hyperparameters.items():\n",
    "    model_name = value.get(\"model_name\", key)\n",
    "    AutoModel.from_pretrained(model_name).save_pretrained(\n",
    "        os.path.join(model_out_dir, f\"{key}-structure\")\n",
    "    )\n",
    "    AutoTokenizer.from_pretrained(model_name).save_pretrained(\n",
    "        os.path.join(model_out_dir, f\"{key}-tokenizer\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ensemble_classifier_dir = os.path.join(model_out_dir, \"bert-based-classifier\")\n",
    "if not os.path.exists(ensemble_classifier_dir):\n",
    "    os.mkdir(ensemble_classifier_dir)\n",
    "for key in hyperparameters.keys():\n",
    "    os.rename(\n",
    "        os.path.join(model_out_dir, f\"{key}-model\", \"model.safetensors\"),\n",
    "        os.path.join(ensemble_classifier_dir, f\"{key}.safetensors\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
