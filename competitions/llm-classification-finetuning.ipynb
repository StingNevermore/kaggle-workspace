{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "data_dir = os.path.join(\"data/llm-classification-finetuning\")\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llm-classification-finetuning-finnal\"\n",
    "os.environ[\"WANDB_DIR\"] = data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "id2label = {0: \"winner_model_a\", 1: \"winner_model_b\", 2: \"winner_tie\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    response_as = examples[\"response_a\"]\n",
    "    response_bs = examples[\"response_b\"]\n",
    "    winner_model_a = examples[\"winner_model_a\"]\n",
    "    winner_model_b = examples[\"winner_model_b\"]\n",
    "    winner_tie = examples[\"winner_tie\"]\n",
    "    ids = examples[\"id\"]\n",
    "\n",
    "    samples = []\n",
    "    for (\n",
    "        prompt,\n",
    "        response_a,\n",
    "        response_b,\n",
    "        winner_model_a,\n",
    "        winner_model_b,\n",
    "        winner_tie,\n",
    "        id,\n",
    "    ) in zip(\n",
    "        prompts,\n",
    "        response_as,\n",
    "        response_bs,\n",
    "        winner_model_a,\n",
    "        winner_model_b,\n",
    "        winner_tie,\n",
    "        ids,\n",
    "    ):\n",
    "        prompt = json.loads(prompt)\n",
    "        response_a = json.loads(response_a)\n",
    "        response_b = json.loads(response_b)\n",
    "        if winner_model_a == 1:\n",
    "            label = \"winner_model_a\"\n",
    "        elif winner_model_b == 1:\n",
    "            label = \"winner_model_b\"\n",
    "        elif winner_tie == 1:\n",
    "            label = \"winner_tie\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label\")\n",
    "\n",
    "        prompt = \"\".join(prompt)\n",
    "        response_a = \"\".join([r if r is not None else \"\" for r in response_a])\n",
    "        response_b = \"\".join([r if r is not None else \"\" for r in response_b])\n",
    "\n",
    "        sentences = [prompt, response_a, response_b]\n",
    "        samples.append((id, sentences, label))\n",
    "\n",
    "    return {\n",
    "        \"id\": [id for id, _, _ in samples],\n",
    "        \"sentences\": [text for _, text, _ in samples],\n",
    "        \"labels\": [label2id[l] for _, _, l in samples],\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_save_dataset(dataset):\n",
    "    dataset = dataset[\"train\"]\n",
    "    dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    # dataset = dataset.shuffle(seed=42).shard(num_shards=100, index=0)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "    dataset.save_to_disk(os.path.join(data_dir, \"dataset_dialog\"))\n",
    "\n",
    "\n",
    "# preprocess_save_dataset(load_dataset(os.path.join(data_dir, \"data_csv\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "import os\n",
    "import torch\n",
    "\n",
    "data_dir = os.path.join(\"data/llm-classification-finetuning\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    encodings = []\n",
    "    for sentence in examples[\"sentences\"]:\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encodings.append(encoding)\n",
    "    result = {}\n",
    "    for key in encodings[0].keys():\n",
    "        result[key] = torch.stack([encoding[key] for encoding in encodings])\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def tokenize_dataset(tokenizer_name):\n",
    "    t = AutoTokenizer.from_pretrained(\n",
    "        tokenizer_name,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    if t.pad_token is None:\n",
    "        t.pad_token = t.eos_token\n",
    "\n",
    "    tokenizer = partial(\n",
    "        tokenize_function,\n",
    "        tokenizer=t,\n",
    "    )\n",
    "    dataset = load_from_disk(os.path.join(data_dir, \"dataset_dialog\"))\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenizer, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class LongSeqClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A long sequence classifier that uses a pre-trained transformer model as the base model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model,\n",
    "        num_classes,\n",
    "        base_model_require_grad=True,\n",
    "        lstm_hidden_size=256,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.model = base_model\n",
    "        self.base_model_require_grad = base_model_require_grad\n",
    "        self.word_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            input_size=lstm_hidden_size * 2,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_classes)\n",
    "\n",
    "        def xavier_init(layer):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if \"weight_ih\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"weight_hh\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"bias\" in name:\n",
    "                    torch.nn.init.zeros_(param.data)\n",
    "                    # 设置forget gate的偏置为1\n",
    "                    param.data[lstm_hidden_size : 2 * lstm_hidden_size].fill_(1)\n",
    "\n",
    "        xavier_init(self.word_lstm)\n",
    "        xavier_init(self.sentence_lstm)\n",
    "        nn.init.uniform_(self.classifier.weight, a=-0.1, b=0.1)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.uniform_(self.classifier.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sentences = input_ids.size(1)\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "        inputs = {\n",
    "            k: v for k, v in locals().items() if k in [\"input_ids\", \"attention_mask\"]\n",
    "        }\n",
    "        if self.base_model_require_grad:\n",
    "            transformer_outputs = self.model(**inputs)\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                transformer_outputs = self.model(**inputs)\n",
    "\n",
    "        outputs = transformer_outputs.last_hidden_state\n",
    "\n",
    "        word_lstm_output, _ = self.word_lstm(\n",
    "            outputs\n",
    "        )  # (batch_size * num_sentences, seq_len, lstm_hidden_size * 2)\n",
    "        sentence_embeddings = []\n",
    "        for i in range(batch_size * num_sentences):\n",
    "            mask = attention_mask[i].bool()\n",
    "            valid_output = word_lstm_output[i][mask]\n",
    "            if len(valid_output) > 0:\n",
    "                sentence_embedding = valid_output.mean(dim=0)\n",
    "            else:\n",
    "                sentence_embedding = torch.zeros(self.word_lstm.hidden_size * 2).to(\n",
    "                    word_lstm_output.device\n",
    "                )\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings).view(\n",
    "            batch_size, num_sentences, -1\n",
    "        )\n",
    "\n",
    "        sentence_lstm_output, _ = self.sentence_lstm(sentence_embeddings)\n",
    "        finnal_output = self.dropout(sentence_lstm_output[:, -1, :])\n",
    "        logits = self.classifier(finnal_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a50c95850924158b52d2da0fecdd820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModel\n",
    "from peft import prepare_model_for_kbit_training, PeftModel\n",
    "\n",
    "bit_and_byte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    quantization_config=bit_and_byte_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(\n",
    "    base_model, gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "model = LongSeqClassifier(base_model, num_classes=3)\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"/home/nevermore/ml-workspace/kaggle-workspace/data/llm-classification-finetuning/output-Meta-Llama-3-8B-4bit-lora/model\",\n",
    ")\n",
    "tokenized_dataset = tokenize_dataset(\"meta-llama/Meta-Llama-3-8B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fefd1154af7f4ad8a2c3bca4a8176063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/719 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "classifier = model\n",
    "\n",
    "classifier.to(torch.device(\"cuda\"))\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "classifier.eval()\n",
    "\n",
    "llama_logits = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = {k: v.to(torch.device(\"cuda\")) for k, v in inputs.items()}\n",
    "        outputs = classifier(**inputs)\n",
    "        llama_logits.append(outputs[1].detach().cpu().numpy())\n",
    "llama_logits = np.concatenate(llama_logits, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1f3eb5097974f2387dca7832786c6d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, AutoModel\n",
    "from peft import PeftModel, prepare_model_for_kbit_training\n",
    "\n",
    "bit_and_byte_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "base_model = AutoModel.from_pretrained(\n",
    "    \"google/gemma-2-2b\",\n",
    "    quantization_config=bit_and_byte_config,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "base_model = prepare_model_for_kbit_training(\n",
    "    base_model, gradient_checkpointing_kwargs={\"use_reentrant\": False}\n",
    ")\n",
    "model = LongSeqClassifier(base_model, num_classes=3)\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    \"/home/nevermore/ml-workspace/kaggle-workspace/data/llm-classification-finetuning/output-gemma-2-2b-4bit-lora/model\",\n",
    ")\n",
    "classifier = model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e3491b42554aa59603c3302ca3b086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import default_data_collator\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "classifier = model\n",
    "\n",
    "classifier.to(torch.device(\"cuda\"))\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=16,\n",
    "    pin_memory=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "classifier.eval()\n",
    "\n",
    "gemma_logits = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = {k: v.to(torch.device(\"cuda\")) for k, v in inputs.items()}\n",
    "        outputs = classifier(**inputs)\n",
    "        gemma_logits.append(outputs[1].detach().cpu().numpy())\n",
    "gemma_logits = np.concatenate(gemma_logits, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979836762643524 1.1193134470481056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       message: Optimization terminated successfully.\n",
       "       success: True\n",
       "        status: 0\n",
       "           fun: 0.9964845564404504\n",
       "             x: [ 3.100e+00 -6.333e-01]\n",
       "           nit: 51\n",
       "          nfev: 102\n",
       " final_simplex: (array([[ 3.100e+00, -6.333e-01],\n",
       "                       [ 3.100e+00, -6.333e-01],\n",
       "                       [ 3.100e+00, -6.332e-01]]), array([ 9.965e-01,  9.965e-01,  9.965e-01]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "labels = np.array(tokenized_dataset[\"test\"][\"labels\"])\n",
    "print(log_loss(labels, softmax(llama_logits)), log_loss(labels, softmax(gemma_logits)))\n",
    "\n",
    "\n",
    "def ensembling(logits_list, W):\n",
    "    return np.sum(\n",
    "        [W[i] * softmax(logits) for i, logits in enumerate(logits_list)], axis=0\n",
    "    )\n",
    "\n",
    "\n",
    "log_loss(labels, softmax(ensembling([llama_logits, gemma_logits], [0.5, 0.5])))\n",
    "\n",
    "\n",
    "minimize(  # minimize log loss\n",
    "    lambda W: log_loss(labels, softmax(ensembling([llama_logits, gemma_logits], W))),\n",
    "    x0=[0.5, 0.5],\n",
    "    method=\"Nelder-Mead\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2883788 , 0.9152256 , 1.2630954 ],\n",
       "       [0.838242  , 1.071621  , 0.55683696],\n",
       "       [0.33881384, 0.97839296, 1.1494932 ],\n",
       "       ...,\n",
       "       [1.9319441 , 0.1695876 , 0.3651681 ],\n",
       "       [0.84046996, 1.0711923 , 0.55503774],\n",
       "       [1.5497681 , 0.354356  , 0.5625758 ]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensembling([llama_logits, gemma_logits], [3.100, -0.6333])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 57477\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/nevermore/.cache/huggingface/accelerate/default_config.yaml')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from accelerate.utils import write_basic_config\n",
    "\n",
    "write_basic_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LongSeqClassifier(\n",
       "      (model): Gemma2Model(\n",
       "        (embed_tokens): Embedding(256000, 2304, padding_idx=0)\n",
       "        (layers): ModuleList(\n",
       "          (0-25): 26 x Gemma2DecoderLayer(\n",
       "            (self_attn): Gemma2Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): Gemma2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Gemma2MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2304, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=9216, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=9216, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): PytorchGELUTanh()\n",
       "            )\n",
       "            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n",
       "      )\n",
       "      (word_lstm): ModulesToSaveWrapper(\n",
       "        (original_module): LSTM(2304, 256, batch_first=True, bidirectional=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): LSTM(2304, 256, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "      )\n",
       "      (sentence_lstm): ModulesToSaveWrapper(\n",
       "        (original_module): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "      (classifier): ModulesToSaveWrapper(\n",
       "        (original_module): Linear(in_features=512, out_features=3, bias=True)\n",
       "        (modules_to_save): ModuleDict(\n",
       "          (default): Linear(in_features=512, out_features=3, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15099494.4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4096 * 4096 * 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "data_csv_split_dir = os.path.join(data_dir, \"data_csv_split\")\n",
    "os.makedirs(data_csv_split_dir, exist_ok=True)\n",
    "df = pd.read_csv(os.path.join(data_dir, \"data_csv\", \"train.csv\"))\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_df.to_csv(os.path.join(data_dir, \"data_csv_split\", \"train.csv\"), index=False)\n",
    "test_df.to_csv(os.path.join(data_dir, \"data_csv_split\", \"test.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51729, 9), (5748, 9))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    pd.read_csv(os.path.join(data_dir, \"data_csv_split\", \"train.csv\")).shape,\n",
    "    pd.read_csv(os.path.join(data_dir, \"data_csv_split\", \"test.csv\")).shape,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57477, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(os.path.join(data_dir, \"data_csv\", \"train.csv\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9138b5d6174a4dc18e0edbad9f9913f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26152e24ae074d2e9145c1de8b1cabd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 51729\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 5748\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset(\"csv\", data_dir=os.path.join(data_dir, \"data_csv_split\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8594bc40333d489ba6cf696cafa083d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 19:43:15,433] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_name = \"google/gemma-2-9b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(os.path.join(data_dir, \"tokenizer\", \"gemma-2-9b\"))\n",
    "model.save_pretrained(os.path.join(data_dir, \"full_precision\", \"gemma-2-9b\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b91f4dba5d054b1f8acbaaeeb3a49aef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeef989d1c64b59a37bfd5bf71a5feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813047bc9a274b6782c29f76e5dcd644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "230660c04a364274bdaa4951e387b0ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fe2df724614aa59554dd43973f08b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5abaf20c6c5f4f6194ead5953e70e177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b5ed8a99484cd5915c58982f1d4106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e1fe855c144ec986599bd3c6c8919c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d17aaa1a3d0a485bab270880c7760106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d73bc3eab6841aa889f434eaffd01f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa74214e18564819b6a49a5c30d72dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer.save_pretrained(os.path.join(data_dir, \"tokenizer\", \"llama-3-8b-instruct\"))\n",
    "model.save_pretrained(os.path.join(data_dir, \"full_precision\", \"llama-3-8b-instruct\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
