{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "data_dir = os.path.join(\"data/llm-classification-finetuning\")\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = \"9994\"  # modify if RuntimeError: Address already in use\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llm-classification-finetuning-finnal\"\n",
    "os.environ[\"WANDB_DIR\"] = data_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "id2label = {0: \"winner_model_a\", 1: \"winner_model_b\", 2: \"winner_tie\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    response_as = examples[\"response_a\"]\n",
    "    response_bs = examples[\"response_b\"]\n",
    "    winner_model_a = examples[\"winner_model_a\"]\n",
    "    winner_model_b = examples[\"winner_model_b\"]\n",
    "    winner_tie = examples[\"winner_tie\"]\n",
    "    ids = examples[\"id\"]\n",
    "\n",
    "    samples = []\n",
    "    for (\n",
    "        prompt,\n",
    "        response_a,\n",
    "        response_b,\n",
    "        winner_model_a,\n",
    "        winner_model_b,\n",
    "        winner_tie,\n",
    "        id,\n",
    "    ) in zip(\n",
    "        prompts,\n",
    "        response_as,\n",
    "        response_bs,\n",
    "        winner_model_a,\n",
    "        winner_model_b,\n",
    "        winner_tie,\n",
    "        ids,\n",
    "    ):\n",
    "        prompt = json.loads(prompt)\n",
    "        response_a = json.loads(response_a)\n",
    "        response_b = json.loads(response_b)\n",
    "        if winner_model_a == 1:\n",
    "            label = \"winner_model_a\"\n",
    "        elif winner_model_b == 1:\n",
    "            label = \"winner_model_b\"\n",
    "        elif winner_tie == 1:\n",
    "            label = \"winner_tie\"\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label\")\n",
    "\n",
    "        prompt = \"\".join(prompt)\n",
    "        response_a = \"\".join([r if r is not None else \"\" for r in response_a])\n",
    "        response_b = \"\".join([r if r is not None else \"\" for r in response_b])\n",
    "\n",
    "        sentences = [prompt, response_a, response_b]\n",
    "        samples.append((id, sentences, label))\n",
    "\n",
    "    return {\n",
    "        \"id\": [id for id, _, _ in samples],\n",
    "        \"sentences\": [text for _, text, _ in samples],\n",
    "        \"labels\": [label2id[l] for _, _, l in samples],\n",
    "    }\n",
    "\n",
    "\n",
    "def preprocess_save_dataset(dataset):\n",
    "    dataset = dataset[\"train\"]\n",
    "    dataset = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=8,\n",
    "        remove_columns=dataset.column_names,\n",
    "    )\n",
    "    # dataset = dataset.shuffle(seed=42).shard(num_shards=100, index=0)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, shuffle=True, seed=42)\n",
    "    dataset.save_to_disk(os.path.join(data_dir, \"dataset_dialog\"))\n",
    "\n",
    "\n",
    "# preprocess_save_dataset(load_dataset(os.path.join(data_dir, \"data_csv\")))\n",
    "dataset = load_from_disk(os.path.join(data_dir, \"dataset_dialog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "\n",
    "class LongSeqClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model: PreTrainedModel,\n",
    "        num_classes,\n",
    "        lstm_hidden_size=256,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super(LongSeqClassifier, self).__init__()\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.base_model = base_model\n",
    "        self.word_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            input_size=lstm_hidden_size * 2,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_classes)\n",
    "\n",
    "        def xavier_init(layer):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if \"weight_ih\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"weight_hh\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"bias\" in name:\n",
    "                    torch.nn.init.zeros_(param.data)\n",
    "                    # 设置forget gate的偏置为1\n",
    "                    param.data[lstm_hidden_size : 2 * lstm_hidden_size].fill_(1)\n",
    "\n",
    "        xavier_init(self.word_lstm)\n",
    "        xavier_init(self.sentence_lstm)\n",
    "        nn.init.uniform_(self.classifier.weight, a=-0.1, b=0.1)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.uniform_(self.classifier.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sentences = input_ids.size(1)\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "        inputs = {\n",
    "            k: v for k, v in locals().items() if k in [\"input_ids\", \"attention_mask\"]\n",
    "        }\n",
    "        self.base_model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.base_model(**inputs).last_hidden_state\n",
    "\n",
    "        word_lstm_output, _ = self.word_lstm(\n",
    "            outputs\n",
    "        )  # (batch_size * num_sentences, seq_len, lstm_hidden_size * 2)\n",
    "        sentence_embeddings = []\n",
    "        for i in range(batch_size * num_sentences):\n",
    "            mask = attention_mask[i].bool()\n",
    "            valid_output = word_lstm_output[i][mask]\n",
    "            if len(valid_output) > 0:\n",
    "                sentence_embedding = valid_output.mean(dim=0)\n",
    "            else:\n",
    "                sentence_embedding = torch.zeros(self.word_lstm.hidden_size * 2).to(\n",
    "                    word_lstm_output.device\n",
    "                )\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings).view(\n",
    "            batch_size, num_sentences, -1\n",
    "        )\n",
    "\n",
    "        sentence_lstm_output, _ = self.sentence_lstm(sentence_embeddings)\n",
    "        finnal_output = self.dropout(sentence_lstm_output[:, -1, :])\n",
    "        logits = self.classifier(finnal_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            return loss, logits\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29f1d5daeabd44b4a9bb3292c9f2835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "identifier = \"llama-3B-orgin\"\n",
    "base_model_name = \"meta-llama/llama-3.2-3B-Instruct\"\n",
    "base_mode = AutoModel.from_pretrained(\n",
    "    base_model_name, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "classifier = LongSeqClassifier(base_mode, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer):\n",
    "    encodings = []\n",
    "    for sentence in examples[\"sentences\"]:\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encodings.append(encoding)\n",
    "    result = {}\n",
    "    for key in encodings[0].keys():\n",
    "        result[key] = torch.stack([encoding[key] for encoding in encodings])\n",
    "    result[\"labels\"] = examples[\"labels\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "t = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    use_fast=True,\n",
    ")\n",
    "if t.pad_token is None:\n",
    "    t.pad_token = t.eos_token\n",
    "tokenizer = partial(\n",
    "    tokenize_function,\n",
    "    tokenizer=t,\n",
    ")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenizer, batched=True, remove_columns=dataset[\"train\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-31 15:05:10,649] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msting_nevermore\u001b[0m (\u001b[33msting_nevermore-personal\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>data/llm-classification-finetuning/wandb/run-20241031_150513-lluvbv6l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal/runs/lluvbv6l' target=\"_blank\">run-llama-3B-orgin</a></strong> to <a href='https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal' target=\"_blank\">https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal/runs/lluvbv6l' target=\"_blank\">https://wandb.ai/sting_nevermore-personal/llm-classification-finetuning-finnal/runs/lluvbv6l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3234' max='3234' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3234/3234 1:43:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>647</td>\n",
       "      <td>1.081500</td>\n",
       "      <td>1.060604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1294</td>\n",
       "      <td>1.053600</td>\n",
       "      <td>1.045273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1941</td>\n",
       "      <td>1.034600</td>\n",
       "      <td>1.031837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2588</td>\n",
       "      <td>1.044200</td>\n",
       "      <td>1.027467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3234, training_loss=1.0577747012334764, metrics={'train_runtime': 6211.4653, 'train_samples_per_second': 8.328, 'train_steps_per_second': 0.521, 'total_flos': 0.0, 'train_loss': 1.0577747012334764, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name=f\"run-{identifier}\",\n",
    "    output_dir=os.path.join(data_dir, f\"output-{identifier}\"),\n",
    "    logging_dir=os.path.join(data_dir, f\"logs-{identifier}\"),\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    save_only_model=True,\n",
    "    save_steps=0.2,\n",
    "    save_strategy=\"steps\",\n",
    "    save_total_limit=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=200,\n",
    "    report_to=[\"tensorboard\", \"wandb\"],\n",
    "    overwrite_output_dir=True,\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    bf16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=classifier,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(os.path.join(data_dir, f\"model-{identifier}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f3c466823348e28526f31ded787262",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/360 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import default_data_collator\n",
    "\n",
    "classifier.to(torch.device(\"cuda\"))\n",
    "\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "classifier.eval()\n",
    "\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = {k: v.to(torch.device(\"cuda\")) for k, v in inputs.items()}\n",
    "        outputs = classifier(**inputs)\n",
    "        logits.append(outputs[1].detach().cpu().numpy())\n",
    "logits = np.concatenate(logits, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0275)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "CrossEntropyLoss()(torch.tensor(logits), torch.tensor(eval_dataset[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'BertForLongSeqClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bert \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mBertForLongSeqClassification\u001b[49m(bert, lstm_hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, drop_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      3\u001b[0m load_model(\n\u001b[1;32m      4\u001b[0m     classifier,\n\u001b[1;32m      5\u001b[0m     os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel-roberta-large\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m classifier\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForLongSeqClassification' is not defined"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained(\"roberta-large\")\n",
    "classifier = BertForLongSeqClassification(bert, lstm_hidden_size=256, drop_out=0.5)\n",
    "load_model(\n",
    "    classifier,\n",
    "    os.path.join(data_dir, \"model-roberta-large\", \"model.safetensors\"),\n",
    "    device=\"cuda\",\n",
    ")\n",
    "classifier.to(torch.device(\"cuda\"))\n",
    "\n",
    "eval_dataset = tokenized_dataset[\"test\"]\n",
    "dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    collate_fn=default_data_collator,\n",
    ")\n",
    "classifier.eval()\n",
    "\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for inputs in tqdm(dataloader):\n",
    "        inputs = {k: v.to(torch.device(\"cuda\")) for k, v in inputs.items()}\n",
    "        outputs = classifier(**inputs)\n",
    "        logits.append(outputs[1].detach().cpu().numpy())\n",
    "logits = np.concatenate(logits, axis=0)\n",
    "roberta_logits = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0628800346190332, 1.0619226026429176)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "labels = np.array(tokenized_dataset[\"test\"][\"labels\"])\n",
    "log_loss(labels, softmax(bert_logits)), log_loss(labels, softmax(roberta_logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.078454545235536"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensembling(logits_list, W):\n",
    "    return np.sum(\n",
    "        [W[i] * softmax(logits) for i, logits in enumerate(logits_list)], axis=0\n",
    "    )\n",
    "\n",
    "\n",
    "log_loss(labels, softmax(ensembling([bert_logits, roberta_logits], [0.5, 0.5])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       message: Optimization terminated successfully.\n",
       "       success: True\n",
       "        status: 0\n",
       "           fun: 1.0600688823375397\n",
       "             x: [-6.653e-01  4.028e+00]\n",
       "           nit: 58\n",
       "          nfev: 119\n",
       " final_simplex: (array([[-6.653e-01,  4.028e+00],\n",
       "                       [-6.653e-01,  4.028e+00],\n",
       "                       [-6.653e-01,  4.029e+00]]), array([ 1.060e+00,  1.060e+00,  1.060e+00]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "minimize(  # minimize log loss\n",
    "    lambda W: log_loss(labels, softmax(ensembling([bert_logits, roberta_logits], W))),\n",
    "    x0=[0.5, 0.5],\n",
    "    method=\"Nelder-Mead\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93de267689d14d0186143aa2a8d803bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4e4aba8bd24acaa7524e86d5f5067d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90afbeeb125e4d39bdf1c3f62624e862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "model = AutoModel.from_pretrained(\"google/gemma-2-9b-it\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2Model(\n",
       "  (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "  (layers): ModuleList(\n",
       "    (0-41): 42 x Gemma2DecoderLayer(\n",
       "      (self_attn): Gemma2Attention(\n",
       "        (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "        (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "        (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "        (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "        (rotary_emb): Gemma2RotaryEmbedding()\n",
       "      )\n",
       "      (mlp): Gemma2MLP(\n",
       "        (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "        (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "        (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "        (act_fn): PytorchGELUTanh()\n",
       "      )\n",
       "      (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "    )\n",
       "  )\n",
       "  (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
