{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting eval.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile eval.py\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModel\n",
    "from peft import PeftModel\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "from typing import Optional\n",
    "from accelerate import Accelerator\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "from transformers import HfArgumentParser, default_data_collator\n",
    "from dataclasses import dataclass, field\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Params:\n",
    "    base_model_dir: str = field()\n",
    "    lora_model_dir: str = field()\n",
    "    dataset_dir: str = field()\n",
    "    output_path: str = field()\n",
    "    load_in_4bit: bool = field(default=True)\n",
    "    batch_size: int = field(default=4)\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    prompts = examples[\"prompt\"]\n",
    "    response_as = examples[\"response_a\"]\n",
    "    response_bs = examples[\"response_b\"]\n",
    "    ids = examples[\"id\"]\n",
    "\n",
    "    samples = []\n",
    "    for (\n",
    "        prompt,\n",
    "        response_a,\n",
    "        response_b,\n",
    "        id,\n",
    "    ) in zip(\n",
    "        prompts,\n",
    "        response_as,\n",
    "        response_bs,\n",
    "        ids,\n",
    "    ):\n",
    "        prompt = json.loads(prompt)\n",
    "        response_a = json.loads(response_a)\n",
    "        response_b = json.loads(response_b)\n",
    "\n",
    "        prompt = \"\".join(prompt)\n",
    "        response_a = \"\".join([r if r is not None else \"\" for r in response_a])\n",
    "        response_b = \"\".join([r if r is not None else \"\" for r in response_b])\n",
    "\n",
    "        sentences = [prompt, response_a, response_b]\n",
    "        samples.append((id, sentences))\n",
    "\n",
    "    return {\n",
    "        \"id\": [id for id, _ in samples],\n",
    "        \"sentences\": [text for _, text in samples],\n",
    "    }\n",
    "\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_seq_length=512):\n",
    "    \"\"\"Tokenize function\"\"\"\n",
    "    encodings = []\n",
    "    for sentence in examples[\"sentences\"]:\n",
    "        encoding = tokenizer(\n",
    "            sentence,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encodings.append(encoding)\n",
    "    result = {}\n",
    "    for key in encodings[0].keys():\n",
    "        result[key] = torch.stack([encoding[key] for encoding in encodings])\n",
    "    return result\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    dataset: Dataset,\n",
    "    tokenizer_name_or_path: str,\n",
    "    max_seq_length: int,\n",
    "    accelerator: Accelerator,\n",
    "):\n",
    "    \"\"\"Preprocess the dataset\"\"\"\n",
    "    t = AutoTokenizer.from_pretrained(tokenizer_name_or_path)\n",
    "    if t.pad_token is None:\n",
    "        t.pad_token = t.eos_token\n",
    "    tokenizer = partial(\n",
    "        tokenize_function,\n",
    "        tokenizer=t,\n",
    "        max_seq_length=max_seq_length,\n",
    "    )\n",
    "\n",
    "    with accelerator.main_process_first():\n",
    "        dataset = dataset.map(\n",
    "            preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=dataset.column_names,\n",
    "        )\n",
    "        dataset = dataset.map(\n",
    "            tokenizer, batched=True, remove_columns=dataset.column_names\n",
    "        )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def prepare_dataset(\n",
    "    dataset_dir: str,\n",
    "    tokenizer_name_or_path: str,\n",
    "    max_seq_length: int,\n",
    "    accelerator: Optional[Accelerator],\n",
    "):\n",
    "    \"\"\"Prepare the dataset\"\"\"\n",
    "    dataset = load_dataset(\"csv\", data_dir=dataset_dir)[\"test\"]\n",
    "    dataset = preprocess_dataset(\n",
    "        dataset, tokenizer_name_or_path, max_seq_length, accelerator\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "class LstmTextClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A long sequence classifier that uses a pre-trained transformer model as the base model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model,\n",
    "        num_classes,\n",
    "        base_model_require_grad=True,\n",
    "        lstm_hidden_size=256,\n",
    "        dropout=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        hidden_size = base_model.config.hidden_size\n",
    "        self.model = base_model\n",
    "        self.base_model_require_grad = base_model_require_grad\n",
    "        self.word_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.sentence_lstm = nn.LSTM(\n",
    "            input_size=lstm_hidden_size * 2,\n",
    "            hidden_size=lstm_hidden_size,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(lstm_hidden_size * 2, num_classes)\n",
    "\n",
    "        def xavier_init(layer):\n",
    "            for name, param in layer.named_parameters():\n",
    "                if \"weight_ih\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"weight_hh\" in name:\n",
    "                    torch.nn.init.xavier_uniform_(param.data)\n",
    "                elif \"bias\" in name:\n",
    "                    torch.nn.init.zeros_(param.data)\n",
    "                    # 设置forget gate的偏置为1\n",
    "                    param.data[lstm_hidden_size : 2 * lstm_hidden_size].fill_(1)\n",
    "\n",
    "        xavier_init(self.word_lstm)\n",
    "        xavier_init(self.sentence_lstm)\n",
    "        nn.init.uniform_(self.classifier.weight, a=-0.1, b=0.1)\n",
    "        if self.classifier.bias is not None:\n",
    "            nn.init.uniform_(self.classifier.bias, -0.1, 0.1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass of the model\n",
    "        \"\"\"\n",
    "        batch_size = input_ids.size(0)\n",
    "        num_sentences = input_ids.size(1)\n",
    "        input_ids = input_ids.view(-1, input_ids.size(-1))\n",
    "        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
    "        inputs = {\n",
    "            k: v for k, v in locals().items() if k in [\"input_ids\", \"attention_mask\"]\n",
    "        }\n",
    "        if self.base_model_require_grad:\n",
    "            transformer_outputs = self.model(**inputs)\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            with torch.no_grad():\n",
    "                transformer_outputs = self.model(**inputs)\n",
    "\n",
    "        outputs = transformer_outputs.last_hidden_state\n",
    "\n",
    "        word_lstm_output, _ = self.word_lstm(\n",
    "            outputs\n",
    "        )  # (batch_size * num_sentences, seq_len, lstm_hidden_size * 2)\n",
    "        sentence_embeddings = []\n",
    "        for i in range(batch_size * num_sentences):\n",
    "            mask = attention_mask[i].bool()\n",
    "            valid_output = word_lstm_output[i][mask]\n",
    "            if len(valid_output) > 0:\n",
    "                sentence_embedding = valid_output.mean(dim=0)\n",
    "            else:\n",
    "                sentence_embedding = torch.zeros(self.word_lstm.hidden_size * 2).to(\n",
    "                    word_lstm_output.device\n",
    "                )\n",
    "            sentence_embeddings.append(sentence_embedding)\n",
    "\n",
    "        sentence_embeddings = torch.stack(sentence_embeddings).view(\n",
    "            batch_size, num_sentences, -1\n",
    "        )\n",
    "\n",
    "        sentence_lstm_output, _ = self.sentence_lstm(sentence_embeddings)\n",
    "        finnal_output = self.dropout(sentence_lstm_output[:, -1, :])\n",
    "        logits = self.classifier(finnal_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
    "\n",
    "\n",
    "def eval_loop(\n",
    "    accelerator: Optional[Accelerator],\n",
    "    base_model_dir: str,\n",
    "    lora_model_dir: str,\n",
    "    load_in_4bit: bool,\n",
    "    dataset_dir: str,\n",
    "    batch_size: int,\n",
    "    output_path: str,\n",
    "):\n",
    "    dataset = prepare_dataset(dataset_dir, base_model_dir, 512, accelerator)\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    base_model = AutoModel.from_pretrained(\n",
    "        base_model_dir,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=quantization_config,\n",
    "    )\n",
    "    classifier = LstmTextClassifier(base_model, num_classes=3)\n",
    "    classifier = PeftModel.from_pretrained(classifier, lora_model_dir)\n",
    "    classifier, dataloader = accelerator.prepare(classifier, dataloader)\n",
    "\n",
    "    def flatten_parameters(model):\n",
    "        if isinstance(model, nn.LSTM):\n",
    "            model.flatten_parameters()\n",
    "\n",
    "    classifier.apply(flatten_parameters)\n",
    "\n",
    "    pbar = tqdm(\n",
    "        total=len(dataloader),\n",
    "        desc=\"Evaluating\",\n",
    "        disable=not accelerator.is_main_process,\n",
    "    )\n",
    "    logits = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            outputs = classifier(**batch)\n",
    "            outputs = outputs.logits\n",
    "            gathered = accelerator.gather_for_metrics(outputs)\n",
    "            if accelerator.is_main_process:\n",
    "                logits.append(gathered.cpu())\n",
    "            pbar.update()\n",
    "        if accelerator.is_main_process:\n",
    "            logits = torch.cat(logits)\n",
    "            test_csv = pd.read_csv(os.path.join(dataset_dir, \"test.csv\"))\n",
    "            df = pd.DataFrame(\n",
    "                {\n",
    "                    \"id\": test_csv[\"id\"],\n",
    "                    \"winner_model_a\": logits[:, 0],\n",
    "                    \"winner_model_b\": logits[:, 1],\n",
    "                    \"winner_tie\": logits[:, 2],\n",
    "                }\n",
    "            )\n",
    "            df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = HfArgumentParser((Params,))\n",
    "    (params,) = parser.parse_args_into_dataclasses()\n",
    "    accelerator = Accelerator()\n",
    "    eval_loop(\n",
    "        accelerator,\n",
    "        params.base_model_dir,\n",
    "        params.lora_model_dir,\n",
    "        params.load_in_4bit,\n",
    "        params.dataset_dir,\n",
    "        params.batch_size,\n",
    "        params.output_path,\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default_config.yaml created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "deep_speed_config_path = os.path.join(os.getcwd(), \"zero_stage3_config.json\")\n",
    "\n",
    "with open(\"default_config.yaml\", \"w\") as f:\n",
    "    f.write(\n",
    "        f\"\"\"\n",
    "compute_environment: LOCAL_MACHINE\n",
    "deepspeed_config:\n",
    " deepspeed_config_file: {deep_speed_config_path}\n",
    " zero3_init_flag: true\n",
    "distributed_type: DEEPSPEED\n",
    "fsdp_config: {{}}\n",
    "machine_rank: 0\n",
    "main_process_ip: null\n",
    "main_process_port: null\n",
    "main_training_function: main\n",
    "num_machines: 1\n",
    "num_processes: 2\n",
    "use_cpu: false\n",
    "\"\"\"\n",
    "    )\n",
    "print(\"default_config.yaml created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero_stage3_config.json created\n"
     ]
    }
   ],
   "source": [
    "with open(\"zero_stage3_config.json\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"\"\"{\n",
    "    \"fp16\": {\n",
    "        \"enabled\": true,\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"allgather_partitions\": true,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"contiguous_gradients\": true\n",
    "    },\n",
    "    \"steps_per_print\": 2000,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": false\n",
    "}\"\"\"\n",
    "    )\n",
    "print(\"zero_stage3_config.json created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-14 14:17:15,782] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:17:21,189] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:17:21,304] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:17:22,075] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-14 14:17:22,075] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-14 14:17:22,312] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|                          | 0/8 [00:00<?, ?it/s]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:10<00:00,  1.30s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 8/8 [00:10<00:00,  1.33s/it]\n",
      "[2024-11-14 14:17:38,165] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-14 14:17:38,165] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:17:38,941] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:17:40,970] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-14 14:17:40,980] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2024-11-14 14:17:41,004] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:17:41,122] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-11-14 14:17:41,122] [INFO] [utils.py:782:see_memory_usage] MA 5.85 GB         Max_MA 5.86 GB         CA 6.19 GB         Max_CA 6 GB \n",
      "[2024-11-14 14:17:41,123] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 89.25 GB, percent = 8.9%\n",
      "[2024-11-14 14:17:41,147] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Parameter Offload: Total persistent parameters: 25741830 in 651 params\n",
      "[2024-11-14 14:17:41,792] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-11-14 14:17:41,792] [INFO] [utils.py:782:see_memory_usage] MA 3.02 GB         Max_MA 6.7 GB         CA 7.04 GB         Max_CA 7 GB \n",
      "[2024-11-14 14:17:41,793] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 93.05 GB, percent = 9.2%\n",
      "[2024-11-14 14:17:41,801] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-14 14:17:41,801] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-14 14:17:41,801] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-14 14:17:41,801] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-14 14:17:41,801] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fac7a4286b0>\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-14 14:17:41,802] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   steps_per_print .............. inf\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   train_batch_size ............. 16\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2024-11-14 14:17:41,803] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }\n",
      "}\n",
      "Evaluating:   0%|                                       | 0/360 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Evaluating: 100%|█████████████████████████████| 360/360 [15:29<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_dir = \"/root/autodl-fs/pretrained/gemma-2-9b-it\"\n",
    "lora_model_dir = \"/root/autodl-tmp/lora/gemma-2-9b-it-llmc\"\n",
    "dataset_dir = \"/root/autodl-fs/datasets/llm-classification\"\n",
    "batch_size = 8\n",
    "gemma_output_path = \"/root/autodl-tmp/workspace/gemma-2-9b-it-predictions.csv\"\n",
    "\n",
    "!accelerate launch --config_file default_config.yaml eval.py \\\n",
    "    --base_model_dir {base_model_dir} \\\n",
    "    --lora_model_dir {lora_model_dir} \\\n",
    "    --dataset_dir {dataset_dir} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --output_path {gemma_output_path} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-14 14:33:16,999] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:33:22,393] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:33:22,444] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-11-14 14:33:23,290] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-14 14:33:23,290] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-11-14 14:33:23,323] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards:   0%|                          | 0/7 [00:00<?, ?it/s]`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:09<00:00,  1.34s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:09<00:00,  1.38s/it]\n",
      "[2024-11-14 14:33:36,977] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-14 14:33:36,977] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:33:37,727] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:33:39,454] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-14 14:33:39,462] [INFO] [logging.py:128:log_dist] [Rank 0] Creating ZeRO Offload\n",
      "[2024-11-14 14:33:39,476] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 14:33:39,660] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-11-14 14:33:39,661] [INFO] [utils.py:782:see_memory_usage] MA 4.45 GB         Max_MA 4.46 GB         CA 4.47 GB         Max_CA 4 GB \n",
      "[2024-11-14 14:33:39,661] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 87.42 GB, percent = 8.7%\n",
      "[2024-11-14 14:33:39,680] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Parameter Offload: Total persistent parameters: 20208646 in 437 params\n",
      "[2024-11-14 14:33:40,102] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-11-14 14:33:40,103] [INFO] [utils.py:782:see_memory_usage] MA 2.28 GB         Max_MA 4.94 GB         CA 4.98 GB         Max_CA 5 GB \n",
      "[2024-11-14 14:33:40,103] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 87.5 GB, percent = 8.7%\n",
      "[2024-11-14 14:33:40,110] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-14 14:33:40,110] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-14 14:33:40,110] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-14 14:33:40,110] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-14 14:33:40,110] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   bfloat16_enabled ............. False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f022a8b8050>\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   fp16_auto_cast ............... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   fp16_enabled ................. True\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   gradient_clipping ............ 0.0\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   loss_scale ................... 0\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-14 14:33:40,111] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   steps_per_print .............. inf\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   train_batch_size ............. 16\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  8\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  False\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   zero_enabled ................. True\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\n",
      "[2024-11-14 14:33:40,112] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": false\n",
      "    }\n",
      "}\n",
      "Evaluating:   0%|                                       | 0/360 [00:00<?, ?it/s]/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "/root/miniconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:911: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:1424.)\n",
      "  result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "Evaluating: 100%|█████████████████████████████| 360/360 [10:47<00:00,  1.80s/it]\n"
     ]
    }
   ],
   "source": [
    "base_model_dir = \"/root/autodl-fs/pretrained/llama-3-8b-instruct\"\n",
    "lora_model_dir = \"/root/autodl-tmp/lora/llama-3-8b-instruct-llmc\"\n",
    "dataset_dir = \"/root/autodl-fs/datasets/llm-classification\"\n",
    "batch_size = 8\n",
    "llama_output_path = \"/root/autodl-tmp/workspace/llama-3-8b-instruct-predictions.csv\"\n",
    "\n",
    "!accelerate launch --config_file default_config.yaml eval.py \\\n",
    "    --base_model_dir {base_model_dir} \\\n",
    "    --lora_model_dir {lora_model_dir} \\\n",
    "    --dataset_dir {dataset_dir} \\\n",
    "    --batch_size {batch_size} \\\n",
    "    --output_path {llama_output_path} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def get_labels():\n",
    "    test_df = pd.read_csv(os.path.join(dataset_dir, \"test.csv\"))\n",
    "    test_df[\"labels\"] = test_df.apply(\n",
    "        lambda row: 0\n",
    "        if row[\"winner_model_a\"] == 1\n",
    "        else 1\n",
    "        if row[\"winner_model_b\"] == 1\n",
    "        else 2,\n",
    "        axis=1,\n",
    "    )\n",
    "    return test_df[\"labels\"].values\n",
    "\n",
    "\n",
    "def ensembling(logits_list, W):\n",
    "    return np.sum(\n",
    "        [W[i] * softmax(logits) for i, logits in enumerate(logits_list)], axis=0\n",
    "    )\n",
    "\n",
    "\n",
    "def get_logits(submission_path):\n",
    "    df = pd.read_csv(submission_path)\n",
    "    return df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0005517163000228\n",
      "1.0148125257681215\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "       message: Optimization terminated successfully.\n",
       "       success: True\n",
       "        status: 0\n",
       "           fun: 0.9949258105664686\n",
       "             x: [ 9.294e-01  2.841e+00]\n",
       "           nit: 41\n",
       "          nfev: 80\n",
       " final_simplex: (array([[ 9.294e-01,  2.841e+00],\n",
       "                       [ 9.293e-01,  2.841e+00],\n",
       "                       [ 9.293e-01,  2.841e+00]]), array([ 9.949e-01,  9.949e-01,  9.949e-01]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment to calculate ensemble weights\n",
    "\n",
    "# from sklearn.metrics import log_loss\n",
    "# from scipy.optimize import minimize\n",
    "\n",
    "# labels = get_labels()\n",
    "# gemma_logits = get_logits(gemma_output_path)\n",
    "# llama_logits = get_logits(llama_output_path)\n",
    "\n",
    "\n",
    "# print(log_loss(labels, softmax(gemma_logits)))\n",
    "# print(log_loss(labels, softmax(llama_logits)))\n",
    "\n",
    "# minimize(  # minimize log loss\n",
    "#     lambda W: log_loss(labels, softmax(ensembling([llama_logits, gemma_logits], W))),\n",
    "#     x0=[0.5, 0.5],\n",
    "#     method=\"Nelder-Mead\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_logits = get_logits(llama_output_path)\n",
    "gemma_logits = get_logits(gemma_output_path)\n",
    "\n",
    "test_df = pd.read_csv(os.path.join(dataset_dir, \"test.csv\"))\n",
    "finnal_logits = softmax(ensembling([llama_logits, gemma_logits], [9.294e-01, 2.841e00]))\n",
    "pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test_df[\"id\"],\n",
    "        \"winner_model_a\": finnal_logits[:, 0],\n",
    "        \"winner_model_b\": finnal_logits[:, 1],\n",
    "        \"winner_tie\": finnal_logits[:, 2],\n",
    "    }\n",
    ").to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
